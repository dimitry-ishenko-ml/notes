{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Part-of-Speech Tagging (POS)\n",
    "\n",
    "## Using Hidden Markov Model (HMM) and Viterbi Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, lower):\n",
    "    data_x, data_y = [], []\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "    words, tags = [\"--n--\"], [\"--s--\"]\n",
    "    for line in lines:\n",
    "        if line.split():\n",
    "            word, tag = line.split()\n",
    "            word = word.strip()\n",
    "\n",
    "            words.append(word.lower() if lower else word)\n",
    "            tags.append(tag.strip())\n",
    "\n",
    "        else:\n",
    "            data_x.append(words)\n",
    "            data_y.append(tags)\n",
    "            words, tags = [\"--n--\"], [\"--s--\"]\n",
    "\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "\n",
    "noun_suffix = [ \"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\" ]\n",
    "verb_suffix = [ \"ate\", \"ify\", \"ise\", \"ize\" ]\n",
    "adj_suffix  = [ \"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\" ]\n",
    "adv_suffix  = [ \"ward\", \"wards\", \"wise\" ]\n",
    "\n",
    "def assign_unk(word):\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return \"--unk_digit--\"\n",
    "    elif any(char in punct for char in word):\n",
    "        return \"--unk_punct--\"\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return \"--unk_upper--\"\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    return \"--unk--\"\n",
    "\n",
    "def oov_to_unk(words, vocab):\n",
    "    return [ word if word in vocab else assign_unk(word) for word in words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(train_path, test_path, vocab_size, lower):\n",
    "    train_x, train_y = read_file(train_path, lower=lower)\n",
    "    test_x, test_y = read_file(test_path, lower=lower)\n",
    "\n",
    "    # get counts for all words in the train set\n",
    "    vocab = Counter()\n",
    "    for words in tqdm(train_x):\n",
    "        for word in words:\n",
    "            vocab[word] = vocab[word] + 1\n",
    "\n",
    "    # grab `vocab_size` most common words\n",
    "    vocab = { word for word, _ in vocab.most_common(vocab_size) }\n",
    "\n",
    "    # replace words not in the vocab with `--unk--...`\n",
    "    train_x = [ oov_to_unk(words, vocab) for words in train_x ]\n",
    "    test_x = [ oov_to_unk(words, vocab) for words in test_x ]\n",
    "\n",
    "    # re-create vocab based on the updated train set\n",
    "    vocab = sorted([*{ word for words in train_x for word in words }])\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matrices(data_x, data_y, vocab, alpha=1e-4):\n",
    "\n",
    "    # compute counts\n",
    "    transition_count, emission_count, tag_count = {}, {}, {}\n",
    "    for words, tags in tqdm(zip(data_x, data_y), total=len(data_x)):\n",
    "\n",
    "        prev = None\n",
    "        for word, tag in zip(words, tags):\n",
    "\n",
    "            if prev is not None:\n",
    "                transition_count[(prev, tag)] = transition_count.get((prev, tag), 0) + 1\n",
    "                emission_count[(tag, word)] = emission_count.get((tag, word), 0) + 1\n",
    "            \n",
    "            tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "            prev = tag\n",
    "\n",
    "    # compute transition matrix\n",
    "    tags = [ *tag_count.keys() ]\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    A = np.zeros((n_tags, n_tags))\n",
    "    for i, tag_from in enumerate(tags):\n",
    "        total_count = tag_count[tag_from] + alpha * n_tags\n",
    "\n",
    "        for j, tag_to in enumerate(tags):\n",
    "            tran = (tag_from, tag_to)\n",
    "            count = transition_count.get(tran, 0)\n",
    "\n",
    "            A[i, j] = (count + alpha) / total_count\n",
    "\n",
    "    # compute emission matrix\n",
    "    n_words = len(vocab)\n",
    "\n",
    "    B = np.zeros((n_tags, n_words))\n",
    "    for i, tag_from in enumerate(tags):\n",
    "        total_count = tag_count[tag_from] + alpha * n_words\n",
    "\n",
    "        for j, word_to in enumerate(vocab):\n",
    "            emis = (tag_from, word_to)\n",
    "            count = emission_count.get(emis, 0)\n",
    "\n",
    "            B[i, j] = (count + alpha) / total_count\n",
    "\n",
    "    return A, B, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words, A, B, tag_map, vocab_map):\n",
    "    n_words = len(words)\n",
    "    n_tags = len(tag_map)\n",
    "    s_idx = tag_map[\"--s--\"]\n",
    "\n",
    "    C = np.zeros((n_tags, n_words))\n",
    "    D = np.zeros((n_tags, n_words), dtype=np.int)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        # init step\n",
    "        if i == 0:\n",
    "            for tag, j in tag_map.items():\n",
    "                if A[s_idx, j] == 0: C[j, i] = -np.inf\n",
    "                else: C[j, i] = log(A[s_idx, j]) + log(B[j, vocab_map[word]])\n",
    "\n",
    "        # forward steps\n",
    "        else:\n",
    "            for tag, j in tag_map.items():\n",
    "                best_prob = -np.inf\n",
    "                best_path = None\n",
    "\n",
    "                for tag_prev, k in tag_map.items():\n",
    "                    prob = C[k, i-1] + log(A[k, j]) + log(B[j, vocab_map[word]])\n",
    "\n",
    "                    if prob > best_prob:\n",
    "                        best_prob = prob\n",
    "                        best_path = k\n",
    "\n",
    "                C[j, i] = best_prob\n",
    "                D[j, i] = best_path\n",
    "\n",
    "    # backward steps\n",
    "    preds = [ C[:, -1].argmax() ]\n",
    "    for i in reversed(range(n_words)): preds.insert(0, D[preds[0], i])\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_x, A, B, tags, vocab):\n",
    "    tag_map = { tag: idx for idx, tag in enumerate(tags) }\n",
    "    vocab_map = { word: idx for idx, word in enumerate(vocab) }\n",
    "\n",
    "    pred_y = []\n",
    "    for words in tqdm(data_x):\n",
    "        preds = viterbi(words[1:], A, B, tag_map, vocab_map)\n",
    "        pred_y.append([ tags[pred] for pred in preds ])\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "def calc_accuracy(true_y, pred_y):\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for words, preds in zip(true_y, pred_y):\n",
    "        total += len(words)\n",
    "        correct += np.sum([ word == pred for word, pred in zip(words, preds) ])\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "source": [
    "## Train parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"WSJ_02-21.pos\"\n",
    "test_path = \"WSJ_24.pos\"\n",
    "\n",
    "vocab_size = 20000\n",
    "to_lower=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 39832/39832 [00:00<00:00, 92514.41it/s]\n",
      "\n",
      "train set size: 39832\n",
      "test set size: 1346\n",
      "\n",
      "vocab size: 20008\n",
      "vocab: ['!', '#', '$', '%', '&', \"'\", \"''\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\"] ... ['young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeros', 'zinc', 'zip', 'zone', 'zones', 'zoning', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, vocab = read_corpus(train_path, test_path, vocab_size, to_lower)\n",
    "\n",
    "print(\"\\ntrain set size:\", len(train_x))\n",
    "print(\"test set size:\", len(test_x))\n",
    "\n",
    "print(\"\\nvocab size:\", len(vocab))\n",
    "print(\"vocab:\", vocab[:20], \"...\", vocab[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 39832/39832 [00:01<00:00, 32225.17it/s]\n",
      "\n",
      "A shape: (46, 46)\n",
      "B shape: (46, 20008)\n",
      "\n",
      "number of tags: 46\n",
      "tags: ['--s--', 'IN', 'DT', 'NNP', 'CD', 'NN', '``', \"''\", 'POS', '(', 'VBN', 'NNS', 'VBP', ',', 'CC', ')', 'VBD', 'RB', 'TO', '.', 'VBZ', 'NNPS', 'PRP', 'PRP$', 'VB', 'JJ', 'MD', 'VBG', 'RBR', ':', 'WP', 'WDT', 'JJR', 'PDT', 'RBS', 'WRB', 'JJS', '$', 'RP', 'FW', 'EX', 'SYM', '#', 'LS', 'UH', 'WP$']\n"
     ]
    }
   ],
   "source": [
    "# compute transmission (A) and emission (B) matrices for Viterbi algorithm\n",
    "A, B, tags = compute_matrices(train_x, train_y, vocab)\n",
    "\n",
    "print(\"\\nA shape:\", A.shape)\n",
    "print(\"B shape:\", B.shape)\n",
    "\n",
    "print(\"\\nnumber of tags:\", len(tags))\n",
    "print(\"tags:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1346/1346 [02:03<00:00, 10.91it/s]\n",
      "accuracy: 0.9550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = predict(test_x, A, B, tags, vocab)\n",
    "acc = calc_accuracy(test_y, pred_y)\n",
    "\n",
    "print(f\"\\naccuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}